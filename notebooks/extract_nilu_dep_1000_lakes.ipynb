{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd89b3-a839-47a6-96e1-2f85dbf741a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rio\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434c721-233f-40e4-9285-20752794e42a",
   "metadata": {},
   "source": [
    "# Extract deposition for 2017 to 2021 for the 1000 Lakes\n",
    "\n",
    "See e-mail from Øyvind Kaste received 14.10.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb30874-083b-4552-8435-007389c43925",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fold = r\"/home/jovyan/shared/critical_loads/raster/deposition\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec08e76-607c-4273-b84e-d8106de8907f",
   "metadata": {},
   "source": [
    "## 1. EMEP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800e195-4933-4b80-ae24-c3ff5f55348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Max\n",
    "emep_dict = {}\n",
    "pars = [\"NHy\", \"NOx\", \"SOx\"]\n",
    "for par in pars:\n",
    "    csv_path = os.path.join(data_fold, \"from_max_2022\", f\"SiteDep{par}.csv\")\n",
    "    df = pd.read_csv(csv_path, skiprows=2)\n",
    "    emep_dict[par] = df\n",
    "    print(f\"{len(df)} stations for {par}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b271707-f27f-4ee8-b19e-620509dd9c6c",
   "metadata": {},
   "source": [
    "## 2. NILU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9312d-ba72-4fe0-b7b6-4c056fb46ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# met.no CRS used by NULI\n",
    "met_crs = r\"+proj=lcc +lat_0=63 +lon_0=15 +lat_1=63 +lat_2=63 +no_defs +R=6.371e+06\"\n",
    "\n",
    "# Example met.no dataset with same geo transfrom etc. (because this wasn't specified by NILU)\n",
    "met_ds = xr.open_dataset(\n",
    "    r\"https://thredds.met.no/thredds/dodsC/metpplatest/met_forecast_1_0km_nordic_latest.nc\"\n",
    ")\n",
    "\n",
    "# Read NILU 1km data\n",
    "pars = [\"nh4\", \"no3\", \"xso4\"]\n",
    "years = range(2017, 2022)\n",
    "ds = xr.Dataset({})\n",
    "for par in pars:\n",
    "    # Create empty DataArray with correct dims\n",
    "    da = xr.DataArray(\n",
    "        dims=(\"y\", \"x\", \"year\"),\n",
    "        coords={\"x\": met_ds.x, \"y\": met_ds.y, \"year\": years},\n",
    "    )\n",
    "    da.rio.write_nodata(np.nan, inplace=True)\n",
    "\n",
    "    # Add NILU data from numpy arrays\n",
    "    for year in years:\n",
    "        arr_path = os.path.join(\n",
    "            data_fold,\n",
    "            \"from_nilu_2022\",\n",
    "            \"nilu_dep_2017-21_met_1km_grid\",\n",
    "            f\"tot_dep_{par}_{year}.npy\",\n",
    "        )\n",
    "        data = np.load(arr_path)\n",
    "\n",
    "        # Add values to DataArray\n",
    "        da.loc[dict(year=year)] = np.flip(data, axis=0)\n",
    "\n",
    "    # Add DataArray to DataSet\n",
    "    ds[par] = da\n",
    "\n",
    "# Update attrs with spatial info from met dataset\n",
    "ds.rio.write_crs(met_crs, inplace=True,).rio.set_spatial_dims(\n",
    "    x_dim=\"x\",\n",
    "    y_dim=\"y\",\n",
    "    inplace=True,\n",
    ").rio.write_coordinate_system(inplace=True)\n",
    "ds.rio.write_transform(met_ds.rio.transform())\n",
    "\n",
    "# Wite example to GeoTIFF for testing in ArcGIS\n",
    "# ds['nh4'].sel(year=2017).rio.to_raster(\"test.tif\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb5353-c4b3-4bbd-bd64-a7d66dfc8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Some stations lie just outside of met's grid. Interpolate using 'nearest neighbour'\n",
    "# (otherwise xarray retrun NaN for ~60 points). Takes about 10 mins\n",
    "ds2 = ds.transpose(\"year\", \"y\", \"x\").rio.interpolate_na(method=\"nearest\")\n",
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c97ad-4652-4665-bf25-d73bbd2e8e92",
   "metadata": {},
   "source": [
    "## 3. Extract values for points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63a427-4d0a-4649-bc68-ce5a7f7582bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get station co-ords from Max's data\n",
    "stn_df = emep_dict[\"NHy\"][[\"StationCode\", \"Latitude\", \"Longitude\"]]\n",
    "stn_gdf = gpd.GeoDataFrame(\n",
    "    stn_df,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        stn_df[\"Longitude\"], stn_df[\"Latitude\"], crs=\"epsg:4326\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Get x and y in grid co-ords\n",
    "stn_gdf[\"x\"] = stn_gdf.to_crs(met_crs)[\"geometry\"].x\n",
    "stn_gdf[\"y\"] = stn_gdf.to_crs(met_crs)[\"geometry\"].y\n",
    "stn_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be118882-b068-499c-8bbd-49336c0de95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values for points\n",
    "x_indexer = xr.DataArray(\n",
    "    stn_gdf[\"x\"], dims=[\"StationCode\"], coords=[stn_gdf[\"StationCode\"]]\n",
    ")\n",
    "y_indexer = xr.DataArray(\n",
    "    stn_gdf[\"y\"], dims=[\"StationCode\"], coords=[stn_gdf[\"StationCode\"]]\n",
    ")\n",
    "pts_ds = ds2.sel(x=x_indexer, y=y_indexer, method=\"nearest\")\n",
    "pts_df = pts_ds.to_dataframe().reset_index()\n",
    "display(pts_df.describe())\n",
    "\n",
    "# Reformat to have similar structure to Max's data\n",
    "pts_df = pts_df[[\"StationCode\", \"year\"] + pars]\n",
    "pts_df = pts_df.melt(id_vars=[\"StationCode\", \"year\"])\n",
    "pts_df.set_index([\"StationCode\", \"year\", \"variable\"], inplace=True)\n",
    "pts_df = pts_df.unstack(\"year\")\n",
    "pts_df.columns = pts_df.columns.get_level_values(1)\n",
    "pts_df.columns.name = \"\"\n",
    "pts_df.reset_index(inplace=True)\n",
    "pts_df.rename({\"StationCode\": \"station_code\"}, inplace=True, axis=\"columns\")\n",
    "pts_df = pts_df.round(1)\n",
    "\n",
    "# Add units (assuming mg/m2, like in Max's data, but not actually specified by NILU\n",
    "pts_df[\"variable\"] = pts_df[\"variable\"] + \"_mg/m2\"\n",
    "\n",
    "# Save\n",
    "pts_df.to_csv(\"../data/1000_lakes_nilu_dep_2017-21.csv\", index=False)\n",
    "\n",
    "pts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76b546-8919-4aac-917b-d2350a2890a7",
   "metadata": {},
   "source": [
    "## 4. Join to EMEP data\n",
    "\n",
    "Øyvind would like the new NILU values appended to Max's data (as `2017_NILU`, `2018_NILU` etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7924f31-62cd-4754-bd06-07fc919ae1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "par_map = {\"NHy\": \"nh4_mg/m2\", \"NOx\": \"no3_mg/m2\", \"SOx\": \"xso4_mg/m2\"}\n",
    "for emep_par, nilu_par in par_map.items():\n",
    "    emep_df = emep_dict[emep_par]\n",
    "    nilu_df = pts_df.query(\"variable == @nilu_par\").copy()\n",
    "    assert len(nilu_df) == 1003\n",
    "    del nilu_df[\"variable\"]\n",
    "    nilu_df.columns = [\"StationCode\"] + [f\"{year}_NILU\" for year in range(2017, 2022)]\n",
    "    df = pd.merge(emep_df, nilu_df, how=\"left\", on=\"StationCode\")\n",
    "    df.to_csv(\n",
    "        f\"../data/{emep_par}_mgpm2_1000_lakes_dep_2017-21_emep_nilu_combined.csv\", index=False\n",
    "    )\n",
    "    df.rename({\"!StationID\": \"StationID\"}, inplace=True, axis=\"columns\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
